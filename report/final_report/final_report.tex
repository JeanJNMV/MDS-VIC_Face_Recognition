\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig} 
\usepackage{graphicx}
\usepackage{amsmath} 
\usepackage{amssymb} 

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{todonotes}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Classical Face Recognition Under Real-World Variations}

\author{
Ayoub EL KBADI \and Fotios KAPOTOS \\ CentraleSupélec \and Jean-Vincent MARTINI 
}


\maketitle
%\thispagestyle{empty}

%%%%%%%%% BODY TEXT

\begin{abstract}
    Face recognition is a classical problem in computer vision that predates modern deep learning approaches and relies on explicit feature extraction and linear subspace modeling. This report presents a comparative study of three well-established appearance-based face recognition methods: Eigenfaces (PCA) \cite{turk1991eigenfaces}, Fisherfaces (LDA) \cite{belhumeur1997fisherfaces}, and Local Binary Pattern Histograms (LBPH) \cite{ahonen2004lbp}. Using the ORL (AT\&T) dataset \cite{samaria1994orl}, the Yale Face Dataset \cite{georghiades2001yale}, and a small custom in-the-wild dataset, we evaluate these methods under controlled and realistic acquisition conditions. The evaluation focuses on robustness to common image degradations, including illumination changes, additive noise, blur, and partial occlusions, as well as sensitivity to limited training data. Quantitative results are complemented by qualitative analyses such as subspace visualizations, confusion matrices, and representative success and failure cases, providing insight into the strengths and limitations of global versus local visual representations. Overall, the study highlights the interpretability, failure modes, and practical relevance of non-deep-learning face recognition techniques. The full code is available at \url{https://github.com/fotisk07/VIC-Project}.
\end{abstract}

\section{Introduction and Motivation}

Face recognition is a fundamental problem in computer vision with applications ranging from access control, security, identity verification, to human - computer interaction. The goal of face recognition is to automatically identify or verify a person based on visual information captured by devices such as cameras. We will not focus on deep learning-based methods, which have become dominant in recent years, but rather on classical appearance-based techniques which are of practical interest due to their interpretability, low computational requirements, and minimal training data requirements.

This project focues on three well-established classical face recognition methods: Eigenfaces, based on Principal Component Analysis (PCA) \cite{turk1991eigenfaces}; Fisherfaces, based on Linear Discriminant Analysis (LDA) \cite{belhumeur1997fisherfaces}; and Local Binary Pattern Histograms (LBPH) \cite{ahonen2004lbp}, which encode local texture information. These methods embody fundamentally different modeling philosophies, ranging from global linear subspace representations to local, texture-based descriptors. % It serves as a final assessment for the course "Introduction to Visual Computing" offered at CentraleSupélec.

We aim to provide a comprehensive comparative evaluation of these methods under varying image acquisition conditions and degradations, such as illumination changes, noise, blur, and partial occlusions. The goal is to determine how robust are these classical techniques to real-world variations and how their performance degrades under those conditions. We will also analyze their sensitivity to limited training data, which is a common practical constraint.

Understanding these questions is important not only for appreciating the evolution of face recognition techniques but also for informing the design of lightweight, interpretable systems in constrained environments or low-data scenarios. Potential applications may include embedded systems, mobile devices, or baseline models for benchmarking and analysis. 
\todo{pas sûr la si vous avez des idées}

\section{Problem Definition} % A METTRE AVEC INTRO ??

We consider a supervised face recognition problem defined over a labeled dataset of face images. Let 
\begin{equation*}
    \mathcal{D} = \{(x_i, y_i)\}_{i=1}^N
\end{equation*}
denote a dataset of $N$ grayscale face images, where $x_i \in \mathbb{R}^{H \times W}$ represents the $i$-th image of height $H$ and width $W$, and $y_i \in \{1, 2, \ldots, C\}$ is the corresponding identity label indicating one of $C$ distinct individuals. Each image is assumed to be preprocessed to a standard size and aligned such that facial features are approximately centered, but may vary in terms of illumination, pose, expression, noise level, or partial occlusions.

Given a training subset $\mathcal{D}_{train} \subset \mathcal{D}$, the task is to learn a mapping function
\begin{equation*}
    f: \mathbb{R}^{H \times W} \rightarrow \{1, 2, \ldots, C\}
\end{equation*}
that predicts the identity label of previously unseen test images. 

To evaluate robustness, we consider a family of image degradation operators 
\begin{equation*}
    \delta_{\alpha}: \mathbb{R}^{H \times W} \rightarrow \mathbb{R}^{H \times W}
\end{equation*}
parameterized by a severity level $\alpha$, modeling effects like illumination changes, additive Gaussian noise, blur, rotation, etc. The primarily objective is always to maximize recognition accuracy
\begin{equation*}
    \text{Accuracy} = \frac{1}{|\mathcal{D}_{test}|} \sum_{(x_j, y_j) \in \mathcal{D}_{test}} \mathbb{I}(f(\delta_{\alpha}(x_j)) = y_j)
\end{equation*}
on the test set $\mathcal{D}_{test}$ and to analyze how this accuracy degrades with respect to the severity $\alpha$ of the applied degradations. Other way of analyzing the results will include confusion matrices, visualizations of learned subspaces, and representative success and failure cases.
\todo{estce que j'en ai trop fais la ?}

\section{Related Work}

\section{Methodology}
\subsection{Face Recognition Algorithms}

\subsubsection{Local Binary Pattern Histograms (LBPH)}
\subsection{Datasets}
We conduct our experiments on two standard face recognition datasets that are widely used in
the evaluation of classical appearance-based methods.

\paragraph{ORL (AT\&T) Face Dataset.}
The ORL face dataset~\cite{samaria1994orl} contains images of 40 individuals, with 10 grayscale
images per subject. The images exhibit moderate variations in facial expression, pose, and the
presence of accessories such as glasses. Due to its controlled nature and limited variability,
this dataset is well suited for analyzing baseline performance and the impact of training set
size on recognition accuracy.

\paragraph{Yale Face Dataset.}
The Yale face dataset~\cite{georghiades2001yale} includes frontal face images captured under
systematically varying illumination conditions. This dataset is particularly challenging for
global appearance-based methods and is therefore well suited for studying robustness to lighting
changes. It provides a classical benchmark for evaluating the effectiveness of discriminative
subspace methods and local texture-based representations. 

\paragraph{Custom In-the-Wild Face Dataset.}
In addition to standard benchmarks, we collect a small custom face dataset composed of images of the project members acquired in unconstrained, real-world conditions.
The images exhibit significant variability in illumination, background, pose, facial expression, image quality, and partial occlusions.
Although limited in size, this dataset provides a qualitative stress test for classical face recognition methods and highlights their limitations outside controlled acquisition settings.
It complements the ORL and Yale datasets by exposing failure modes that arise in more realistic scenarios.
\todo{j'ai juste copié collé du proposal, besoin de + ?}

\section{Evaluation}
\section{Conclusions}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\newpage
{\Huge Proposal} (to be removed later)
\setcounter{section}{0}
\section{Motivation and Problem Definition}

Face recognition is a fundamental problem in computer vision with applications in security,
identity verification, and human - computer interaction.

In this project, we focus on appearance-based face recognition techniques developed prior to
deep learning. These methods rely on explicit feature extraction and linear subspace modeling.

The objective of this project is to compare three classical face recognition methods:
\begin{itemize}
    \item Eigenfaces (PCA-based)\cite{turk1991eigenfaces}
    \item Fisherfaces (LDA-based)\cite{belhumeur1997fisherfaces}
    \item Local Binary Pattern Histograms (LBPH) \cite{ahonen2004lbp}
\end{itemize}
and to analyze their robustness under realistic image degradations such as illumination changes,
noise, blur, and partial occlusions.

\textbf{Problem statement:}
Given a labeled face image dataset, how do different classical face recognition algorithms perform
under varying acquisition conditions, and what do their successes and failures reveal about global
versus local visual representations?

% =====================================================
\section{Related Work}
% =====================================================

The Eigenfaces method introduced by Turk and Pentland applies Principal Component Analysis (PCA)
to project face images into a low-dimensional subspace that captures the main modes of variation.
While effective in controlled settings, Eigenfaces are highly sensitive to illumination changes.

Fisherfaces extend this approach by using Linear Discriminant Analysis (LDA) to maximize class
separability, leading to improved robustness under varying lighting conditions, provided that
sufficient training samples are available.

Local Binary Pattern Histograms (LBPH) represent faces using local texture descriptors computed
over small neighborhoods. This local representation has been shown to be more robust to lighting
variations and partial occlusions, at the expense of increased sensitivity to noise.


% =====================================================
\section{Methodology}
% =====================================================

\subsection{Algorithms}

We will implement and evaluate the following methods:
\begin{itemize}
    \item \textbf{Eigenfaces}: PCA-based dimensionality reduction followed by nearest-neighbor classification.
    \item \textbf{Fisherfaces}: LDA applied after PCA to improve class discrimination.
    \item \textbf{LBPH}: Local Binary Pattern feature extraction with histogram-based comparison.
\end{itemize}


\subsection{Datasets}

We conduct our experiments on two standard face recognition datasets that are widely used in
the evaluation of classical appearance-based methods.

\paragraph{ORL (AT\&T) Face Dataset.}
The ORL face dataset~\cite{samaria1994orl} contains images of 40 individuals, with 10 grayscale
images per subject. The images exhibit moderate variations in facial expression, pose, and the
presence of accessories such as glasses. Due to its controlled nature and limited variability,
this dataset is well suited for analyzing baseline performance and the impact of training set
size on recognition accuracy.

\paragraph{Yale Face Dataset.}
The Yale face dataset~\cite{georghiades2001yale} includes frontal face images captured under
systematically varying illumination conditions. This dataset is particularly challenging for
global appearance-based methods and is therefore well suited for studying robustness to lighting
changes. It provides a classical benchmark for evaluating the effectiveness of discriminative
subspace methods and local texture-based representations.

\paragraph{Custom In-the-Wild Face Dataset.}
In addition to standard benchmarks, we collect a small custom face dataset composed of images of the project members acquired in unconstrained, real-world conditions.
The images exhibit significant variability in illumination, background, pose, facial expression, image quality, and partial occlusions.
Although limited in size, this dataset provides a qualitative stress test for classical face recognition methods and highlights their limitations outside controlled acquisition settings.
It complements the ORL and Yale datasets by exposing failure modes that arise in more realistic scenarios.


\section{Evaluation}
The proposed methods are evaluated using a combination of quantitative and qualitative analyses,
with an emphasis on robustness and interpretability rather than raw recognition performance.
\paragraph{Experimental Protocol.}
For each dataset, we vary the number of training images per subject in order to study sample
efficiency and sensitivity to limited supervision.
Test images are kept fixed across experiments to ensure fair comparison between methods.
When applicable, results are averaged over multiple random training splits.

To assess robustness, controlled degradations are applied to the test images only.
These include illumination changes, additive Gaussian noise, image blur, and partial occlusions.
Each degradation is applied at increasing levels of severity, yielding robustness curves that
characterize performance decay under progressively more challenging conditions.

\paragraph{Evaluation Metrics.}
Performance is primarily measured using recognition accuracy, complemented by confusion matrices
to analyze class-specific failure patterns.
Robustness curves plot recognition accuracy as a function of degradation strength, providing a
compact visualization of stability under adverse conditions.

\paragraph{Qualitative Analysis.}
Beyond quantitative metrics, we perform qualitative analysis by visualizing learned Eigenfaces
and Fisherfaces, as well as representative success and failure cases.
These visualizations help interpret what variations are captured by global subspace methods
(illumination, identity, or noise) and contrast them with the locality-driven behavior of LBP-based
descriptors, thereby explaining observed performance trends.

\paragraph{Reference Comparison with Deep Learning Methods.}
For reference, we also include a comparison with a lightweight deep learning-based face recognition model.
This comparison is not intended to achieve state-of-the-art performance, but to provide contextual insight into the gap between classical appearance-based methods and modern learned representations, particularly under unconstrained acquisition conditions.
The deep learning model is evaluated using the same experimental protocol and test sets when possible, and its results are reported solely as a point of reference.


% =====================================================
\section{Expected Contributions}
% =====================================================

The project aims to deliver:
\begin{itemize}
    \item A reproducible experimental comparison of classical face recognition methods
    \item An analysis of global versus local visual representations
    \item Clean, well-documented code and experimental protocols
\end{itemize}




\end{document}
